\chapter{Constructing a Cardiac Simulation Toolkit}
\label{chapter:toolkit}

The cardiac simulation toolkit was developed to provide a more systematic
framework for exploring the properties of cardiac cell and tissue models.
It does this through offering a uniform and simple cell interface, a series of
standardised pacing protocols and facilities for two dimensional simulations.
It is intended to be easy to script to make constructing more complex numerical
experiments as easy as possible.

Existing cardiac simulation toolkits offer graphical interfaces and require
specification of the desired protocol through such interfaces.
The toolkit instead has inbuilt knowledge of several simulation protocols.
This ensures different investigations use the same protocols, allowing results
to be compared more directly.
In addition, by performing specific simulation protocols with a dedicated
program, performance optimisations specific to a given protocol are possible.

The toolkit proposed in this chapter allows many aspects of the simulation,
including the parameters used in the model, to be specified on the commandline
or via simple configuration files.
By allowing such control potentially boring and time consuming investigations,
such as investigating how a property varies with the alteration of a parameter
or set of parameters, can be driven externally rather than via repeated manual
alterations.
This both reduces errors and increases productivity.

The toolkit therefore offers an extensible environment for performing cardiac
simulations.
It has inbuilt knowledge of several experimental protocols.
No existing simulation toolkit offers these features.


\section{Simulation Environment}

The simulation environment provided by the cardiac toolkit is intended to be as
portable as possible, so that numerical experiments may be run on whichever
platforms are appropriate.  To this end, all the data input structures are based
on open standards, or simple binary formats.  The output formats provided by the
various driver programs are also in simple binary or ASCII formats, to allow
them to be easily visualized with both commercial and open source visualisation
tools.  The results presented later in this chapter were performed on desktop
computers with a Athlon X2 3600+ chip and 1 GB RAM and on Horace, the local HPC
facility.  Horace has 24 compute nodes, each one consisting of four Intel Itanium2
Montecito Dual Core 1.6GHz processors, 16GB RAM and up to 512GB of local scratch
space.  The nodes are connected by a high speed Quadrics QsNetII
interconnect.  Horace provides compilers for both Fortran and C,
and for both the MPI and OpenMP parallelization libraries.

\subsection{Implementation}

The experimental protocol drivers and the cellular models were implemented in
the C programming language.
Much of the supporting code and supplementary tools were implemented in the ruby
programming language~\cite{Flanagan2008}.
The cellular models currently implemented are based on the Hodgkin-Huxley
formalism, although there is no fundamental reason why a Markov chain based
model could not be included.
Inter-cellular coupling for propagation of excitation over a strand or tissue
was implemented using the monodomain equations.

\subsubsection{Cellular Models}

The cellular model used for much of the developmental process was the
Courtemanche et al. human atrial myocyte model~\cite{CRN98}.
Also currently implemented is the four variable formulation of the Fenton-Karma
minimal variable model~\cite{Bueno-Orovio2008}.
These cellular models describe the behaviour of a cell using coupled systems of
non-linear ordinary differential equations.
The ODEs represent the concentrations of intra- and extra-cellular ion species
and the flow of current through ionic channels in the cell membrane or between
intra-cellular compartments, or their notional equivalents in the case of
minimal variable models.

These equations were integrated using the simplest time-stepping method
available, the forward Euler method.
To improve performance and stability, gating variables were integrated using the
Rush-Larsen method.
Whilst this does require a small timestep, the resulting models are relatively
simple, making expansion easy.


\subsubsection{Monodomain Equation}

The monodomain equation ($\S$\ref{sec:intro:math:mono}) was used to couple multiple cells together to describe
a tissue over which excitation could be conducted.
The rate of change of membrane potential, $V$, is given by
\begin{equation}
\label{eqn:toolkit:monodomain}
\frac{\partial V}{\partial t} = D \nabla^{2} V - \frac{\ii{ion}}{C_{m}}
\end{equation}
where $D$ is a constant representing the diffusivity of transmembrane potential through
space, \ii{ion}\ represents the total trans-membrane of a cellular model, such
as \ii{tot}\ from (\ref{eqn:intro:math:crn}) and $C_{m}$ is the membrane capacitance
of the cell.
A finite differences approach is used to discretize the model in 1D or 2D with
an explicit Euler scheme used to advance the timestep.

\subsubsection{Strand Model}

% \begin{figure}
% \setlength{\unitlength}{1mm}
% \begin{picture}(130,30)
% 
% \multiput(0,10)(20, 0){7}{\usebox{\cell}}
% \multiput(10,14)(20, 0){6}{\usebox{\resistor}}
% \end{picture}
% \caption[Schematic diagram of a 1D strand]{\label{fig:toolkit:strand}
% Schematic diagram of a 1D strand.
% The strand is 7 nodes long, with each node represented by the blue square.
% The electrical activity at each node is represented by a mathematical model of
% the trans-membrane currents.
% The cell is coupled to its neighbours through resistances, the black rectangles.
% }
% \end{figure}

The 1D strand model is used for several experimental protocols, as
a computationally cheaper alternative to a full tissue model.
The 1D strand model consists of a number of nodes, typically 200 or 300, which
are coupled electrically at the ends of the cells.
The electrical activity at each node is modelled via a cellular
electrophysiology model.
Electrical conduction between the nodes is handled via a 1D formulation of the
monodomain equations using a 3-node approximation of the Laplacian, with no
flux boundary conditions.



\subsubsection{Sheet Model}

% \begin{figure}
% \setlength{\unitlength}{1mm}
% \begin{picture}(130,130)
% \multiput(0,0)(20, 0){7}{
%     \multiput(0,0)(0,20){7}{
%         \usebox{\cell}
%     }
% }
% \multiput(10,4)(20, 0){6}{
%     \multiput(0,0)(0, 20){7}{
%         \usebox{\resistor}
%     }
% }
% \multiput(4, 10)(20, 0){7}{
%     \multiput(0, 0)(0, 20){6}{
%         \usebox{\vresistor}
%     }
% }
% \end{picture}
% \caption[Schematic diagram of a 2D sheet]{\label{fig:toolkit:sheet}
% Schematic diagram of a 2D strand.
% The strand is $7\times7$ nodes in size, with each node represented by a blue
% square.
% The electrical activity at each node is represented by a mathematical model of
% the trans-membrane currents.
% The cell is coupled to its neighbours along the cardinal directions through
% resistances, the smaller black rectangles.
% }
% \end{figure}

The 2D sheet model is used for several experimental protocols, as well as more
general numerical experimentation.  The 2D sheet model consists of a grid of
nodes, coupled electrically along the cardinal directions of the grid.  The
electrical activity at each node is modelled via a cellular electrophysiology
model.  Conduction of the electrical excitation between the nodes uses the
monodomain equations with no flux boundary conditions applied at all tissue
boundaries and a 5-node approximation of the Laplacian.  The square sheet model,
used in several of the numerical experimental protocols described later, is
typically $375\times375$ nodes, representing 140,625 `cells'.  Two dimensional
idealizations of physiological preparations can have many more nodes, to on the
order of $10^{6}$ `cells'.  These idealizations can often be quite
irregular and so to allow easy and effective partition of workload across
multiple processors, the tissue map is decomposed into a 1D array which contains
references to the neighbouring cells.


\subsection{Parallelization}

Some parts of the toolkit require the modelling of large numbers of cells, on
the order of tens or even hundreds of thousands of cellular models in two
dimensional sheets.  Solving all the equations involved takes a significant
amount of time and so it is desirable for such simulations to be parallelized so
that the work involved can be split over several processors.  This can have
advantages beyond merely having eight rather than one cores worth of
computational cycles working on solving the equations.  Splitting the work over
multiple cores can also increase the amount of cache available, allowing for
more efficient operation of the solvers.
There are two libraries widely available for parallelization,
OpenMP~\cite{OpenMP}\ and the Message Passing Interface~\cite{MPI} (MPI).
They are based around different paradigms for parallelism.

The OpenMP library is based around the `shared memory' paradigm.
Under the shared memory paradigm, there is only one process and this has access
to all of the memory used in the program.
The parallelism is achieved through the use of threads which divide up the work
between themselves.
This makes the implementation quite simple, but also limits the maximum number
of computer cores which can be assigned to work on any individual execution of
the problem.

MPI is based around the `message passing' paradigm.
The message passing paradigm involves multiple separate processes which use
communication via the messages they pass between themselves to work.
Each process has its own memory, and the only way for information in one process
to reach another process is by explicitly passing it through messages.
Because the processes each have their own memory, there is no requirement that
the processes are on the same computer.
This allows (theoretically) any number of cores to be applied to solving the
problem.
However, the explicit nature of message passing can make implementing a program
harder and in addition, there can be significant lag due to the physical
separation of the computers which can reduce efficiency.

For the parallelism involved in the library, the OpenMP parallelism library was
chosen.
This choice was made for a number of reasons.
Implementations of OpenMP can be found on many systems, with gcc~\cite{gcc},
icc~\cite{icc}\ and the
Sun compiler all having an OpenMP implementation.
This makes the library suitable for use on both HPC systems and on modern
multiple core desktops.
In addition, for those systems on which running in parallel is not desirable,
the same code can be compiled serially.
Whilst an MPI implementation would allow more processors to be used to execute a
task, in general 8 cores of Horace were found to be sufficient for running most
jobs and in compiling for MPI some of the flexibility would be lost from the
library.

\subsubsection{Parallel Fraction and Amdahl's Law}

Ideally, parallelizing computer code would speed the execution up in direct
proportion to the number processors employed.
Use four processors and the computation is completed in one quarter of the time.
Unfortunately, this relationship rarely holds.
This was first noted by Amdahl~\cite{Amdahl1967}, although his observations were
not directly related to parallelization, rather to the general idea of speedup.
Since differing portions of the code do not speed up in the same way, the
maximum speedup that can be observed will be limited by the lowest speedup.

For parallel code, this concept becomes the parallel fraction.
Some, hopefully most, of the code will be executed in parallel.
This code will speed up when run on more processors.
Some of the code will be serial however.
This is typically communication, both between parallel processes and to output
data, but might also include set up costs.
Serial code can't get faster on more computers, without an algorithm change, and
so it limits the maximum speed up.
The maximum speedup, $S_N$, for $N$ processors is given by
\begin{equation}
\label{eqn:toolkit:amdahl}
S_N = \frac{1}{\left(1 - P\right) + \frac{P}{N}}
\end{equation}
where $P$ is the parallel fraction.

\subsection{Performance Optimisation}

Optimisation is the process of improving a given quantity.
In the context of performance, this involves reducing the running time whilst
preserving the accuracy of the solution.
The benefits of this should be obvious.
Code which runs faster lets results be gathered sooner or allows more cases to
be considered.
Optimisation can be of particular benefit to a library, which is intended to
facilitate code re-use; the benefit of a little work can be garnered many
times.

In general, all performance optimisation techniques involve reducing the number
of operations required to compute the final result.
This can take a number of forms.
The simplest one is the choice of the compiler and the compile flags used, both
of which can have a significant influence on the total computation time.
However, moving beyond the compiler, a choice of algorithm can also be important
in reducing the time taken.
Several of the techniques used are presented here.

\subsubsection{The Compiler}

The library has been compiled using the GNU C compiler (gcc)~\cite{gcc}\ and the
Intel C compiler (icc)~\cite{icc}.
Both have OpenMP implementations available and are capable of
performing a number of optimisations, controlled via flags.
The most important aspect of the optimisations is that they should not alter the
behaviour of the floating point handling, as this could have significant impact
on the final result computed.
Despite this caveat, the results of applying certain optimisation flags can be
quite significant, speeding execution by several percent.

\subsubsection{Caching of Computed Values}

Moving beyond the compiler, one of the simplest forms of optimisation is to only
calculate each value once, if at all possible.  This can be done in a number of
ways and the library developed here implements two such methods for saving
computational time.

State saving is one of the most direct ways of caching computed values.  At a
particular point in the simulation, all of the state variables of the
system are copied into an intermediate location.  This might be a file on disk or
to another location in memory.  If the state is written out to a file, that file can
be used as a `save point', allowing the simulation to be continued from that
point in the future, ensuring work is not wasted.

When copied to another memory location, this allows the program to return to
that point in the future.  This is useful in modelling many experimental
protocols, which often call for a number of `conditioning' pulses to allow the
cell or model to settle.  The state can be saved after the conditioning pulses
and then the actual tests can be performed quickly, saving the execution of
several seconds of simulated activity.  This technique should obviously only be
used for cells in the Hodgkin-Huxley formalism which are deterministic and thus
give identical results whether the state is saved or not.  Using such a
technique with a cell that has a number of stochastic components could
potentially affect the quality of the results.

The speedup gained by this method of saving state can be quite significant, but
it varies markedly depending on the protocol used.
To give a concrete example, the conduction velocity restitution protocol,
described in detail later, involves pre-pacing the strand ten times before the
conduction velocity is measured with an eleventh stimulus.
This is repeated perhaps forty times with different intervals between the tenth
and eleventh stimuli, to track the restitution curve.
This requires the strand to simulated through 440 stimulations.
The first nine pre-pacing intervals will be identical in each of those forty
cases (to allow for premature stimuli, the tenth stimulus must be repeated).
If the state is saved just before the tenth stimulus is delivered, then instead
only 89 stimulations need to be calculated.
The nine prepacing stimuli, the result of which is saved, and then the forty
pairs of tenth and eleventh stimuli.
This gives a speedup of almost 5x.
In practice this might be better as the test
stimulus is often premature, reducing the total time that must be stimulated for
the pairs of tenth and eleventh stimuli.


The second way in which caching can be employed is in the creation of `lookup
tables'~\cite{Victorri1985,Cooper2006}.
A lookup table is a pre-computed table of the values an expression can
take.
When the expression would normally be evaluated, the table is used
instead, replacing what might be a complicated expression with a single array
lookup.

To efficiently pre-compute values for a lookup table, two things must be true.
The tabulated expression must depend on only one variable.
The tabulated expression must also be sufficiently `complex' or time consuming
to compute.
The requirement for complexity is perhaps the most obvious one.
The most time that a lookup table can save is the cost of the original
computation, so for significant savings in computational time, expensive
calculations should be preferred.
Good candidates for this are expressions which involve the computation of
mathematical logs and exponentials.
The requirement for dependence on only one variable is due to the nature of the
table.
It must be indexed by the steps in the dependent variable or variables.
If there are $N$ steps in a dependent variable to be tabulated, adding
dependence on a second variable requires pre-computing and storing
$N\,\times\,N$ values.
The value of $N$ varies, but is typically at least 1000.

With these limitations in mind, there are still a number of expressions that can
be tabulated in the typical electrophysiological cell model.
If the Rush-Larsen method has been used to integrate gating variables then the
expressions for both steady state and time constant of the gate can typically be
tabulated.
Currents with a complex, but wholly voltage dependent form, such as \ii{K1}\ in
the Courtemanche et al. human atrial cell model can have the current calculation
tabulated.
Other calculations in a cell model must be evaluated on a case-by-case basis.

The use of lookup tables can significantly speed up code execution.
It can also influence the results, due to the discretisation involved in the
computations.
However even a small number of steps, sufficient to discretise with a
resolution of \mv{0.1}, typically introduces a $\leq$~0.1\% error.
A typical example of the difference lookup tables can make to execution time is
shown in table~\ref{tbl:toolkit:lookup}.
A speed up of 2.9x is attained for the simulation of a plane wave on a
$375\times375$ sheet of CRN cells.

\begin{table}
    \caption[Execution time with and without lookup tables]{
    Execution time for simulations with and without lookup tables.  Both
    simulations were performed on one processor on horace, solving a plane wave
    on a $375\times375$ node sheet of CRN cells for \ms{100}.
    A speedup of almost 3x is attained.
    }
    \begin{center}
    \begin{tabular}{ l l }
    \toprule
    Case & Execution Time (seconds) \\
    \midrule
    no lookup tables & 3750.5 \\
    lookup tables & 1284.0 \\
    \bottomrule
    \label{tbl:toolkit:lookup}
    \end{tabular}
    \end{center}
\end{table}

\subsubsection{Binary Searches}

Several of the experimental protocols provided by the library are intended to
determine the value of a parameter which causes a particular condition to be
fulfilled, such as a successful excitation of the cellular model after
progressively shortening stimulus intervals.  This value we will call the
critical value. In real experiments, ones involving actual cardiac tissue, the
typical experimental protocol would involve stimulating the tissue at
sequentially shorter intervals, until no stimulation was provoked.  This might
involve stimulating the cell thousands of times, which would be expensive
computationally to model exactly.  Instead, a binary search~\cite{IntroAlgo} for the critical
value can be performed, using the pseudo-code shown in Algorithm~\ref{toolkit:binary}

\begin{algorithm}
\caption{
Binary search for the critical value of the function $f(x)$.
The critical value is defined as the smallest $x$ which still makes $f(x)$
produce a value, $v$, greater than the threshold, $t$.
The initial guesses for $x$ are $high$ and $low$.
The guessing continues until sufficiently close for the accuracy condition to be
fulfilled.
}
\label{toolkit:binary}
\begin{algorithmic}
\STATE $x_{high} = high$
\STATE $x_{low} = low$
\REPEAT
\STATE $x_{current} = \left(x_{high} - x_{low}\right) / 2$
\STATE $v = f\!\left(x_{current}\right)$
\COMMENT{Compute $v$ using $x_{current}$}
\IF {$v \geq t$}
\STATE $x_{high} = x_{current}$
\ELSE
\STATE $x_{low} = x_{current}$
\ENDIF
\UNTIL{$\left(x_{high} - x_{low}\right) \leq accuracy$}
\end{algorithmic}
\end{algorithm}

To explain in words, first two guesses are made; the high guess, which is the
maximum value that the critical value can take, and the low guess, the minimum
it is presumed to take.  The simulation is then run with the parameter set at
the average of the low and high guesses--the current guess.  If the test is
successful, the critical value evidently lies somewhere between the low guess
and the average, and so the high guess is set to the current guess.  Conversely,
if the test is unsuccessful, the critical value is obviously above the current
guess, and so the low guess is set to the current guess.  The simulation is then
repeated with the average of the new high and low guess.  Using this algorithm,
the search space is halved with each iteration, swiftly finding the critical
value.
For example, to find a parameter to an accuracy of \ms{1}\ in a range of
\ms{250}\ statistically requires 125 sequential iterations.
To find that same parameter using binary iterations requires just 8.
This is a speedup of approximately 15x, although this depends on where in the
range searched the parameter actually lies.

An important limitation of the binary search method is that there must only be
one critical value.
If there are two such values within the range, the result of the algorithm is
unpredictable.
In practice, this constraint is quite easy to work within.

\subsubsection{Adaptive Step for Restitution Tracking}

Adaptive step mechanisms are employed in the library when there is a need to
provide output over a wide range of times, when the slope of the graph is not
constant over the range to be graphed.  This is very common in the modelling of
cardiac cells, which often show an exponential dependence of various parameters
on the  stimulus interval, and are graphed over a range of hundreds or thousands
of milliseconds.  A step sufficient to track the curve at the upper limits
of the range will completely fail at the steeper slope of the lower limits,
whilst a step that will track the curve for the lower limits will result in
unnecessary work being done at the upper end of the range.  To alleviate this
problem, an adaptive stepping mechanism is used, as shown in the pseudo-code
Algorithm~\ref{toolkit:adaptive}.

\begin{algorithm}
\caption{
Adaptive stepping algorithm for calculating a value, $v$, for decreasing values
of time, $t$ with the function $f\!\left(t\right)$.
$t$ starts at $t_{max}$ and is computed until $t_{min}$.
The initial step size used to reduce $t$ is $step$.
}
\label{toolkit:adaptive}
\begin{algorithmic}
\STATE $step = step_{start}$
\STATE $factor = \frac{step}{2}$
\STATE $v_{last} = f\!\left(t_{max}\right)$
\STATE $t_{prev} = t$
\STATE $t = t_{max} - step$
\WHILE{$t \geq t_{min}$}
    \STATE $v = f\!\left(t\right)$

    \IF {$|v - v_{last}| \leq threshold$}
        \PRINT $t, v$
        \STATE $v_{last} = v$
        \STATE $t_{prev} = t$
        \STATE $t = t - step$
    \ELSE
        \STATE $step = step - factor$
        \STATE $factor = \frac{factor}{2}$
        \STATE $t = t_{prev} - step$
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

First, the measurement is performed at the largest desired point.  The interval
is then reduced by the step, and the measurement is performed again.  The
difference in the measurements is calculated and compared to the desired maximum
delta.  If the difference is acceptable, the interval is once more reduced by
the step, and the measurement taken once more.  If the difference is too great,
then instead the step size is halved and the measurement repeated.  If the
difference is now acceptable, then the interval is reduced by the new step and
the experiment proceeds.  If it is not, then the step size is once more halved.
The step size used is therefore always appropriate to the slope of the curve and
a smooth graph results.  Additional logic, not shown in the pseudo-code, is used
to ensure the step size does not become too small, and to terminate the graph at
the lower end of the range.

Since curves can increase or decrease the absolute difference between the two
values is compared.

\subsubsection{Parallel Speedup}

Single cell and one dimensional problems and protocols are speed up sufficiently
by the techniques previously outlined.
The two dimensional sheet simulations can take a significant amount of time
however.
It also often difficult to apply some of the more significant optimisations such
as binary searches to two dimensional simulations.
For this reason, the two dimensional simulations are parallelized with OpenMP.

As noted earlier, the maximum speedup which can be obtained is limited by
Amdahl's law.
One of the typical serial portions of the code is the input and output; it is
essential if the simulations are to be of any use, but it also typically halts
execution while it is ongoing.
In the toolkit, there are three things periodically output in sheet simulations: a record of all the
membrane potentials, a GIF formatted visualisation of the same, and (less
frequenty) a complete output of all the state variables to allow simulation to
be resumed.
In the toolkit, these are output in parallel with each output initiated in a
different thread of the simulation.
This allows what is normally a serial task to be performed in parallel.

Using OpenMP, these parallel i/o techniques and techniques to improve
parallelization (such as giving each thread its own copy of the lookup tables to
reduce memory bandwidth contention), the speedup obtained is shown in
figure~\ref{fig:toolkit:parallel}.
This was for a simulation of a plane wave on a sheet of $375\times375$ nodes,
using the CRN model with lookup tables.
The simulation was run for \ms{500}\ of simulated time and had output every
\ms{2.5}\ and full state output every \ms{10}.
The graph shows the parallel fraction is somewhere between 0.98 and 0.99 for
eight processors (the limit for an OpenMP process on the horace supercomputer).
This is a good parallel fraction.
The reduced parallel fraction at lower processors could be due to the reduced
size of caches, requiring more information to be accessed from main RAM.

\begin{figure}
\centering
\includegraphics{figures/toolkit/speedup/speed}
\caption[Parallel speedup for toolkit code]{
\label{fig:toolkit:parallel}
Plot of relative speedup against number of processors for a two dimensional
simulation using the toolkit performed on horace.
The relative speedup obtained is plotted in a solid black line.
The solid red line represents ideal (1:1) speedup.
The five dashed red lines represent the speedup predicted for, from top to
bottom, $P = 0.99, 0.98, 0.97, 0.96, 0.95$.
The speedup obtained on 8 processors is very close to that predicted for a
parallel fraction of 0.99.
}
\end{figure}

\section{Experimental Protocols}
\label{sec:toolkit:protocols}

The toolkit developed provides a number of experimental protocols to use with
the cellular models to quantify the electrophysiological behaviour of the
modelled cells.  The provided protocols include the action potential duration at
90\% repolarization (\apd) and the action potential (AP) profile; the \apdr\ and
\apdr[50]\ restitution; the effective refractory period (ERP) restitution
(ERP\emph{r}); the conduction velocity (CV) restitution (CV\emph{r}); the
temporal vulnerability window to unidirectional conduction block (VW); the
threshold of excitation and a flexible system for specifying two dimensional
sheet experiments, including the initiation of re-entry via wavebreak protocols
and computation of the spatial vulnerability window.

\subsection{Action Potential}

\begin{figure}
\centering
\includegraphics{figures/toolkit/illustrations/labelled_ap_profile}
\caption[Illustration of AP properties]{
\label{fig:toolkit:illus:ap}
Schematic AP with various action potential properties noted.
Shown are the overshoot (OS), \apd, \apd[50], $\frac{dV}{dt}_{\text{max}}$ and
the resting membrane potential.
}
\end{figure}

The action potential profile is one of the fundamentals of cellular modelling,
with a number of associated properties, illustrated in
figure~\ref{fig:toolkit:illus:ap}.  These include the action potential
duration at 90\% repolarization, \apd; the action potential
duration at 50\% repolarization, \apd[50]; the maximal overshoot, OS; the
upstroke velocity, $\frac{dV}{dt}_{\text{max}}$ and the resting membrane
potential.

To compute these quantities, the cell is paced $N$\ times at given S1 interval.
After another S1 interval, a final AP is elicited from the cell and the
properties are measured.  In addition, it is common to want current and
cellular model state traces over the course of an AP and these can be provided
by the library alongside the membrane potential trace.

\subsection{Action Potential Duration Restitution}

The library calculates the APD\emph{r} via a standard S1--S2 protocol used in both
numerical simulations~\cite{Kim2002,Xie2002,Qu1999,Cherry2004,Cherry2007,Decker2009} and also in physiological
experiments~\cite{Boyett1978,Taggart1996}.  The APD\emph{r} is used
as a measure of how the cell responds to stimulations at different rates.

A single cell protocol to evaluate the ADP\emph{r}\ curve as results gained are
similar to strand protocols~\cite{Xie2002,Decker2009}\ with much reduced
computational effort.
The cellular model is paced $N-1$\ times with a stimulus close to the threshold
value at a given stimulus interval, S1.  At this point, the state is saved for the
paced cells.  The $N$th S1 stimulus is then given, followed by the S2 after a
varying DI, which is reduced via an adaptive step to record the relationship
between DI and the APD of the following AP.  The toolkit also determines useful
parameters such as the maximal slope of the restitution curve, which can be
related to the stability of spiral waves within the tissue.  Both the \apdr\ and
the \apdr[50]\ restitution can be calculated.

\subsection{Effective Refractory Period Restitution}

The ERP\emph{r} is calculated by the library using standard experimental
protocols~\cite{Workman2001,Kharche2008}.
The ERP is defined as the shortest possible stimulus interval, S2, which still
allows a successful AP to be elicited after pacing $N$ times at a pacing
interval S1.  A successful AP is defined as an AP which has an amplitude
of at least 80\% of the magnitude of the preceeding AP.  
The rate dependence of the ERP is evaluated at a decreasing S1 interval.

To find the ERP for a given S1 interval the cellular model is paced $N$ times
at that interval.
The state is saved just before the $N-1^{\text{th}}$ AP is initiated.
The ERP is found via binary search.
The low guess for S2 is typically chosen as zero, whilst the high guess is the
S1 interval being tested.
The S2 interval for each attempt is the average of the high and low guesses.
After the state has been saved, the $N^{\text{th}}$ AP is initiated and its
amplitude recorded.
Then \ms{S2} after, the test AP is evoked.
After the test AP has been allowed to run its course, the amplitude is tested
and used to guide the next binary iteration.
Details of the elicited APs, such the S1 and S2 amplitudes and durations are
stored.
The binary iteration proceeds until the desired accuracy has been attained.

The reduction in S1 interval is stepped via an adaptive mechanism which is
used to keep the reduction in ERP between successive S1 intervals to below
\ms{1}.  The S1 interval is reduced until it is sufficiently short that the
S2 interval would fall within the $N^{\text{th}}$ AP.

\subsection{Vulnerable Window}

The VW measurement is based around a 1D ring model of cardiac tissue.
It is used to quantify the vulnerability of cardiac tissue to the genesis of
arrhythmia via re-entrant activity~\cite{Quan1990,Zhang2003,Gonzalez2003}.
The VW is defined as the time period in the refractory tail of a propagating
excitation wave that results in unidirectional conduction block.
In the case of a ring model of the tissue this causes retrograde propagation,
which forms a re-entrant excitation which cycles endlessly.
If the stimulus is given too early, then the tissue will still be refractory in
both directions and no propagation of excitation will ensue.
If it is given too late, then propagation will occur in both directions, which
in the ring case, results in the two excitation wavefronts annihilating each
other.
Normal pacing could then resume.
These three cases are illustrated in figure~\ref{fig:toolkit:illus:vw}.


\begin{figure}
\centering
\includegraphics{figures/toolkit/illustrations/vw}
\caption[Illustration of VW cases]{
\label{fig:toolkit:illus:vw}
Illustrative space--time plots of the vulnerable window.
In all plots, colour represents membrane potential from resting (blue) to
depolarized (orange), position along the strand changes horizontally, whilst
time advances upwards.
Normal conduction is from left to right.
In all plots, the latter half of the last S1 pulse can be seen in the lower
right.
(a) Complete Block.  The stimulus is too early, and no excitation can be evoked.
(b) Unidirectional Block.
The stimulus has been delivered in the vulnerable window.
The stimulus can propagate only back along the strand.
In a ring, this would be re--entrant.
(c) Bidirectional Conduction.
The stimulus is delivered too late.
The bidirectional waves will anihilate each other.
}
\end{figure}


The VW is found in a 1D strand model, set up as described previously.
The use of a open-ended strand, not a connected ring, makes pacing the strand
easier, but does not effect the results.
The strand is 300 units long and with a space step of \mm{0.1}.
The strand is first given $N$ S1 conditioning stimuli which are administered to a
3 node (\mm{0.3}) section at the start of the strand.
Typically an S1 interval of \ms{1000}\ is used with 10 S1 pulses.
The test S2 stimulus is administered to a 4 node (\mm{0.4}) section, normally
centred in the middle of the strand, \ms{S2}\ after the $N^{\text{th}}$
conditioning excitation wave has passed the S2 stimulus site.
To reduce the computation time, the state is saved at this point.
The low guess for the binary iteration is chosen as \ms{0}\ and the high guess
as the S1 interval.
To judge the success of the S2 stimulus, the ends of the strands are watched for
successful excitation.
If there are no excitation waves crossing the ends after the $N^{\text{th}}$
excitation wave has passed, then it is in the region of total conduction block.
If one excitation wave crosses the end, it is in region of unidirectional block.
When two excitation waves cross the end, it is in the region of bidirectional
conduction.
The timing of the S2 stimulus is controlled via binary search, first to find the
boundary of total conduction block and unidirectional block and then to find the
boundary between unidirectional block and bidirectional conduction.
A minor optimisation above the usual binary search algorithm is possible in this
case, as a search for one boundary can be used to refine the range for the
second boundary too.

\subsection{Threshold of Excitation}

The threshold of excitation is a theoretical measure, proposed by Zhang et
al.~\cite{Zhang2003}\ and used in modelling studies~\cite{Kharche2008}.  It is
defined as the minimum stimulus current which, when delivered to a cell, will
cause the cell to depolarize to a membrane potential of at least \mv{-20}.  The
threshold of excitation is calculated for a range of stimulus intervals,
successively reducing the interval until it is impossible to elicit a
depolarization of sufficient magnitude.  At each stimulus interval it is
recorded if the test pulse elicits bidirectional, unidirectional or no
propagation.

The threshold of excitation is found in a 1D strand model.  The strand is 300
units long, with a space step of \mm{0.1}.  The strand is first given $N$ S1
stimuli at a rate which allows the strand to recover between each excitation
wave.  Each S1 stimulus is delivered to the first 4 nodes (\mm{0.4}) and is
chosen to be above the threshold of excitation.  The threshold of excitation is
calculated at the 100th node and so as this node depolarises, the state for the
whole strand is cached.

The threshold of excitation is then found via binary search, with a lower bound
of \unit{0}{nS} and an upper bound chosen to be 5x the normal threshold.  The
test stimulus is delivered $\Delta t$\ seconds after the 100th node depolarises
to a group of 4 nodes centred on the 100th node.  After the test stimulus is
delivered the 4 nodes are tested for the excitation condition, attaining a
membrane potential of \mv{-20}.  If it is successful, the current stimulus
strength will be assigned to the high guess.  If not, to the low guess.  In
addition, the simulation is continued to evaluate whether bidirectional,
unidirectional or no conduction of the excitation wave is evoked by the
stimulus.  The strand is then reset to the cached state and the new stimulus
strength is tested until a sufficient accuracy has been attained.  Once the
threshold of excitation has been determined for a given $\Delta t$\ the state of
the strand is once more reset to the cached state and a shorter $\Delta t$\
tested.

\subsection{Conduction Velocity Restitution}

The CV is the rate of propagation of the excitation wave.
It is determined by the difference in excitation times at two points divided by
the distance between them.
It is measured both in both experimental and numerical studies and is therefore
useful in validating experimental results.
A related measurement is the minimum conduction interval.
The minimum conduction interval is the shortest interval between an S1 and S2
stimulus which still propagates successfully.
It is similar to the ERP, but can also be influenced by inter-cellular coupling
and heterogeneity in the strand.
The CV\emph{r} is found via stimulating the strand at successively shorter
intervals and noting the changes in the measured
CV~\cite{Cherry2008,Zhang2003,Qu2006}.
The minimum stimulus interval is found via noting when the curve ends.


The CV\emph{r} is found in a 1D strand model, set up as described previously.
The strand is 300 units long with a space step of \mm{0.1}.
Stimuli above the stimulus threshold are delivered to a 4 unit (\mm{0.4}) length
at one end of the strand.
The strand is first given $N$ S1 stimuli.
Typically an S1 interval of \ms{1000}\ is used with 10 S1 pulses.
The S2 stimulus is then delivered \ms{S2} seconds later.
The CV is estimated from the difference in excitation times, defined as the
instant at which the node is excited above \mv{-60}, at 2 nodes which are
located 100 nodes apart.
This minimizes any possible influence from boundary conditions.
The S2 time is then stepped until a second excitation wave
does not propagate the length of the strand.


\subsection{Spiral Wave Dynamics}

The dynamic behaviours of spiral waves are characterised by the stability,
mobility and lifespan (LS).
Spiral Wave LS is examined experimentally~\cite{Kumagai1997} and
numerically~\cite{Qu2000,Nygren2001,Cherry2007,Clayton2005,Zhang2003}.
The LS of the spiral wave and the meander pattern of the tip are both used to
gain insight into the behaviour of the tissue under conditions of cardiac
arrhythmia.

Spiral waves are initiated in a square sheet of tissue $375\,\text{x}\,375$
nodes in dimension with a space step of \mm{0.1}, as described previously.  The
tissue is first stimulated along one edge via a stimulus current applied to a
row of nodes extending the length of the tissue and 3 nodes (\mm{0.3}) in width.
The planar wave is then allowed to propagate over the tissue.  Some time after
the first wave is initiated, a second stimulus is applied.  The second stimulus
is applied to half the tissue, bisecting the propagation front of the first wave.
The second stimulus is a voltage clamp, with all the included tissue clamped to
a `high' potential, typically \mv{+0} for a millisecond.  The
generated spiral is then allowed to evolve until it self-terminates, the spiral
wave tip exits the tissue or until a sufficient amount of time has passed such
that the spiral can be classified as `persistent'.  The time allowed for a wave
to be classified as persistent is typically 5 or \unit{10}{s}.

The spiral wave tip traces are calculated via a standard contour based
algorithm, comparing the \mv{-60} contour line on snapshots of the electrical
activity \ms{2.5} apart.

\subsection{Spatial Vulnerability of Cardiac Tissue}

The spatial vulnerability (SV) of cardiac tissue is defined as the smallest
length of tissue which, when given a stimulus at the threshold level in the wake
of a propagating wave, gives rise to at least one `figure of eight'
re-entry~\cite{Zou2005}.
A figure of eight re-entry occurs when the excitation waves from the ends of the
test length propagate back through the centre of the length.
This results in a pair of contra-rotating spiral waves, one at each end of the
test length.
The SV is useful for quantifying a mutation or condition's potential for
arrhythmogenesis by giving an indication of the size of ectopic focus required
to excite the tissue.
A small SV indicates that the tissue could be very likely to have arrhythmic
episodes.

The sheet model used for the determination of the SVW can vary in size, as the
SVW can vary substantially, depending on the electrophysiology being simulated
by the cellular models at the nodes, but the smallest used is typically
$375\,\text{x}\,375$ nodes, with a spatial resolution of \mm{0.1}.  The sheet is first given one
conditioning excitation, initiated by injecting a strip of nodes 3 nodes
(\mm{0.3}) in width with current along one edge of the sheet.  The wave is then
allowed to propagate through the tissue.  When the VW of the tissue is
positioned at the centre of the tissue, the test stimulus is delivered.  The
test stimulus is an area of tissue 20 nodes (\mm{2}) wide and of variable length.
After the test stimulus is delivered, the sheet is observed until figure of
eight re-entry is observed, or it is obvious that it will not occur.  The
protocol is then repeated with a test stimulus area of greater length.




\section{Discussion and Conclusions}

A cardiac simulation library has been constructed.
This includes implementations of a wide range of experimental protocols used to
classify and quantify the influence on electrophysiology of ion channel changes.
The library of tools was then used in two electrophysiological studies of atrial
tissue.

The cardiac simulation library developed includes experimental protocols used on
lone myocyte models, as well as one- and two-dimensional idealisations of
tissue.
The library of tools is intended to be relatively simple to use and to extend,
although it does require knowledge of programming to use to its full potential
in its present form.
The implementation is intended to be efficient, though without sacrificing
biophysical detail in realistic cell models.
This is accomplished through the use of adaptive stepping methods to track
restitution curves of varying slope, decreasing the gap between data points as
it starts to change more rapidly.
In addition, state saving is performed to reduce the computation involved in
repetitive pre-pacing.
Lookup tables of voltage dependent properties can be employed to further cut
computational time.
The novel use of a very basic computer science algorithm, the binary search, is
used to determine critical values in a number of protocols.
The use of this algorithm allows parameter estimation to great accuracy from
large initial range by successively halving the search space.

Whilst both the binary search and the adaptive step are generally very reliable,
they do on occasion fail dramatically.
This is often caused by incorrect choice of inputs, which don't allow the
algorithm to correctly function, for example, selecting the high and low guesses
for the vulnerable window period too late.
There is unfortunately little that can be done in such cases by the toolkit
itself.
Results might need to be rechecked with a smaller window, centered around the
found time.

The toolkit compares well with many existing frameworks.
COR has many more cells available for use through the CellML repository and
offers an easy way to graph the internal properties of the cell.
COR is limited running on Microsoft windows, whilst the toolkit is limited
to running on linux.
It has no inbuilt experimental protocols, at the time of writing, although there
will eventually be `COR projects' which will enable protocols to be specified
and shared more easily.
All experimental protocols therefore have to be programmed individually for each
cell, either in the programming language provided by COR or the CellML markup
language.
The publicly available version of COR is also limited to single cell
simulations, although papers indicate it can be used for more complex
simulations and includes parallelization.
COR does use a more sophisticated solver than the toolkit developed here, the
CVODE solver.

CMISS lacks experimental protocols and is hard to script to use automatically.
It does offer much more sophisticated visualisation than the toolkit here, and
can be used for full 3D simulations too.
CARP offers much more powerful interfaces for cardiac modelling, including 3D
models.
It includes no experimental protocols, although those could probably be
implemented using its API.
Like this toolkit, CARP does not integrate with CellML.

