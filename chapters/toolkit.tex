\chapter{Constructing a Cardiac Simulation Toolkit}

\section{Simulation Environment}

The simulation environment provided by the cardiac toolkit is intended to be as
portable as possible, so that numerical experiments may be run on whichever
platforms are appropriate.  To this end, all the data input structures are based
on open standards, or simple binary formats.  The output formats provided by the
various driver programs are also in simple binary or ASCII formats, to allow
them to be easily visualized with both commercial and open source visualisation
tools.  The results presented later in this chapter were performed on desktop
computers with a XX GHz Althon X2 chip and 1 GB RAM and on Horace, the local HPC
facility.  Horace has 24 compute nodes, each one consisting of four Intel Itanium2
Montecito Dual Core 1.6GHz processors, 16GB RAM and up to 512GB of local scratch
space.  The nodes are connected by a high speed Quadrics QsNetII
interconnect~\cite{horace}.  Horace provides compilers for both Fortran and C,
and for both the MPI and OpenMP parallelization libraries.

\subsection{Implementation}

The experimental protocol drivers and the cellular models were implemented in
the C programming language, although much of the supporting code and
supplementary tools were implemented in the ruby programming language.  The
cellular models currently implemented are based on the Hodgkin-Huxley formalism,
although there is no fundamental reason why a Markov chain based model could not
be included.  Inter-cellular coupling for propagation of excitation over a
strand or tissue was implemented using the monodomain equations.

\subsubsection{Cellular Models}

The cellular model used for much of the developmental process was the
Courtemanche et al. human atrial myocyte model~\cite{crn98}.  Also currently
implemented are the Nygern et al. human atrial myocyte model~\cite{nygern98} and
the four variable formulation of the Fenton-Karma minimal variable
model~\cite{overo2008}.  These cellular models describe the behaviour of a cell
using coupled systems of non-linear ordinary differential equations.  The ODEs
represent the concentrations of intra- and extra-cellular ion species and the
flow of current through ionic channels in the cell membrane or between
intra-cellular compartments, or their notional equivalents in the case of minimal
variable models.

These equations were generally solved using the simplest time-stepping method
available, the explicit Euler method.  To improve performance and stability,
some variables were integrated using the Rush-Larsen method.  More complex
integration schemes were tried, but did not significantly improve performance to
compensate for the greater complexity.

\subsubsection{Monodomain Equations}

The monodomain equations were used to couple multiple cells together to describe
a tissue over which excitation could be conducted.



The monodomain equations are considerably simpler than the bidomain equations
which have an elliptical component, representing the extra-cellular potentials,
that must be solved via iterative methods.  This additional complexity is not
required unless the desire is to model situations where there is significant
coupling to external voltage sources, for example to model defibrillation
techniques.

\subsubsection{Strand Model}

The 1D strand model is used for several experimental protocols, as
a computationally cheaper alternative to a full tissue model.  The 1D strand
model consists of a number of nodes, typically 200 or 300, which are coupled
electrically at the ends of the cells.  The electrical activity at each node is
modelled via a cellular electrophysiology model and electrical conduction between the
nodes is handled via a 1D formulation of the monodomain equations, with no
flux boundary conditions.

\subsubsection{Sheet Model}

The 2D sheet model is used for several experimental protocols, as well as more
general numerical experimentation.  The 2D sheet model consists of a grid of
nodes, coupled electrically along the cardinal directions of the grid.  The
electrical activity at each node is modelled via a cellular electrophysiology
model.  Conduction of the electrical excitation between the nodes uses the
mono-domain equations with no flux boundary conditions applied at all tissue
boundaries.  The square sheet model, used in several of the numerical
experimental protocols described later, is typically 375x375 nodes, representing
140625 cellular models.  Two dimensional idealizations of physiological
preparations can have many more nodes, to on the order of $10^{6}$ cellular
models.  These idealizations can often be quite irregular and so to allow easy
and effective partition of workload across multiple processors, the tissue map
is decomposed into a 1D array which contains references to the neighbouring
cells.


\subsection{Parallelization}

Some parts of the toolkit require the modelling of large numbers of cells, on
the order of tens or even hundreds of thousands of cellular models in two
dimensional sheets.  Solving all the equations involved takes a significant
amount of time and so it is desirable for such simulations to be parallelized so
that the work involved can be split over several processors.  This can have
advantages beyond merely having eight rather than one cores worth of
computational cycles working on solving the equations.  Splitting the work over
multiple cores can also increase the amount of cache available, allowing for
more efficient operation of the solvers.

For the toolkit, the OpenMP~\cite{OpenMP} parallelization library was chosen.
This implements the shared memory parallelism paradigm.  Using the shared memory
paradigm is simpler, due to the lack of explicit communication calls, and can be
faster, as there is merely memory access involved, rather than communication
over an network or other interconnect.  Finally, a program written using OpenMP
can, with some care, also be compiled serially if an OpenMP compiler is not
available.  Whilst a program implemented using a non-shared memory paradigm,
such as using the MPI library, would allow it to be executed on potentially more
processors which would potentially offer faster run times, in practice the eight
core shared memory system Horace provides as a compute node was general found to
be sufficient for the task.

\subsection{Optimisation}

The toolkit uses several techniques to optimize the computations performed,
reducing the number of processing cycles required to compute the results needed
for publication.  These optimisations are generally built around the principle
of ``No code is faster than no code'', although it might be more accurate to say
``No code execution is faster than no code executed''.  To explain this idea
another way, it is that no combination of compiler flags can compete with a
sensible choice of algorithm which minimizes the number of computational steps
to be performed or the amount of data to be written to a file.  This is not to
say that the compiler is are unimportant, but that it is merely the start of
optimisations, rather than the final step.

\subsubsection{The Compiler}

The toolkit has been compiled using the GNU C compiler (gcc)~\cite{gcc}, the
Intel C compiler (icc)~\cite{icc} and the Sun microsystems .  All three have
OpenMP implementations available and all three are capable of performing a
number of optimisations, controlled via flags.  The most important aspect of the
optimisations is that they should not alter the behaviour of the floating point
handling, as this could have significant impact on the final result computed.
Despite this caveat, the results of applying certain optimisation flags can be
quite significant.

\subsubsection{Caching of Computed Values}

Moving beyond the compiler, one of the simplest forms of optimisation is to only
calculate each value once, if at all possible.  This can be done in a number of
ways and the toolkit developed here implements two such methods for saving
computational time.

State saving is one of the most direct ways of caching computed values.  At a
particular point in the simulation, all of the state variables of the
system are copied into an intermediate location.  This might be a file on disk or
to another location in memory.  If the state is written out to a file, that file can
be used as a `save point', allowing the simulation to be continued from that
point in the future, ensuring work is not wasted.

When copied to another memory location, this allows the program to return to
that point in the future.  This is useful in modelling many experimental
protocols, which often call for a number of `conditioning' pulses to allow the
cell or model to settle.  The state can be saved after the conditioning pulses
and then the actual tests can be performed quickly, saving the execution of
several seconds of simulated activity.  This technique should obviously only be
used for cells in the Hodgkin-Huxley formalism which are deterministic and thus
give identical results whether the state is saved or not.  Using such a
technique with a cell that has a number of stochastic components could
potentially affect the quality of the results.

The second way in which caching can be employed is in the creation of `lookup
tables'.  A lookup table is a pre-computed table of the values an expression can
take.  When the expression would normally be evaluated, the table is used
instead, replacing what might be a complicated expression with a single array
lookup.  For lookup tables to be efficient to pre-compute, the tabulated
expression should depend on only one variable and should be sufficiently
`complex' expression, such as one involving the computation of mathematical logs
or exponentials.  The expression should depend on only one variable as the
pre-computed table has to be indexed over each variable the computation depends
on and even dependence on just two variables would increase the number of table
entries required by thousands or millions.  The requirement for complexity is a
little more obvious, as any optimisation can only save as much time as the
original series of operations took.  With these limitations in mind, it is still
possible to find many candidates for pre-computation in typical cardiac cell
models, most notably the activation and inactivations of voltage dependent
gates.  This technique is typically only worthwhile performing in the case of
tissue simulations where the benefits can be shared amongst all the cells since
pre-computation can be quite expensive, destroying any efficiency gains made
through their use for single cell simulations.

\subsubsection{Binary Searches}

Several of the experimental protocols provided by the toolkit are intended to
determine the value of a parameter which causes a particular condition to be
fulfilled, such as a successful excitation of the cellular model after
progressively shortening stimulus intervals.  This value we will call the
critical value. In real experiments, ones involving actual cardiac tissue, the
typical experimental protocol would involve stimulating the tissue at
sequentially shorter intervals, until no stimulation was provoked.  This might
involve stimulating the cell thousands of times, which would be expensive
computationally to model exactly.  Instead, a binary search for the critical
value can be performed, using the pseudo-code shown here.


To explain in words, first two guesses are made; the high guess, which is the
maximum value that the critical value can take, and the low guess, the minimum
it is presumed to take.  The simulation is then run with the parameter set at
the average of the low and high guesses--the current guess.  If the test is
successful, the critical value evidently lies somewhere between the low guess
and the average, and so the high guess is set to the current guess.  Conversely,
if the test is unsuccessful, the critical value is obviously above the current
guess, and so the low guess is set to the current guess.  The simulation is then
repeated with the average of the new high and low guess.  Using this algorithm,
the search space is halved with each iteration, swiftly finding the critical
value.  For example, to find a parameter somewhere in the range of 0--1000~ms to
the nearest millisecond requires just 10 iterations of the binary search
algorithm, but might take hundreds of iterations with sequential searching.

One important thing that must be considered when using binary searches is that
there is only one critical value in the range considered or else the algorithm
will give unpredictable results.  In practice, this limitation is often quite
easy to work within.

\subsubsection{Adaptive Step}

Adaptive step mechanisms are employed in the toolkit when there is a need to
provide output over a wide range of times, when the slope of the graph is not
constant over the range to be graphed.  This is very common in the modelling of
cardiac cells, which often show an exponential dependence of various parameters
on the  stimulus interval, and are graphed over a range of hundreds or thousands
of milliseconds.  A step that sufficient to track the curve at the upper limits
of the range will completely fail at the steeper slow of the lower limits,
whilst a step that will track the curve for the lower limits will result in
unnecessary work being done at the upper end of the range.  To alleviate this
problem, an adaptive stepping mechanism is used, as shown in this pseudo-code.

First, the measurement is performed at the largest desired point.  The interval
is then reduced by the step, and the measurement is performed again.  The
difference in the measurements is calculated and compared to the desired maximum
delta.  If the difference is acceptable, the interval is once more reduced by
the step, and the measurement taken once more.  If the difference is too great,
then instead the step size is halved and the measurement repeated.  If the
difference is now acceptable, then the interval is reduced by the new step and
the experiment proceeds.  If it is not, then the step size is once more halved.
The step size used is therefore always appropriate to the slope of the curve and
a smooth graph results.  Additional logic, not shown in the pseudo-code, is used
to ensure the step size does not become too small, and to terminate the graph at
the lower end of the range.

Since curves can increase or decrease the absolute difference between the two
values is compared.

\subsubsection{Parallel Input/Output}

Input and output for simulations is obviously essential if they are to be of any
use.  This input and output can involve the reading or writing of many megabytes
or gigabytes of information over the course of a simulation, often in a variety
of different formats.  This most significant for sheet simulations and it is
those that we consider here.

Data input is typically not that significant a cost for
simulations, as whilst various simulation maps and saved states be quite large,
the cost is typically only paid once.  Data output is often required at numerous
points throughout the course of the simulation since almost all simulations are
performed to examine the time evolution of the cardiac system.  Many simulations
will output the value of state parameters, most commonly the voltage, at all
points in the tissue every few milliseconds of simulated time.  Other state
variables might also be desired and the toolkit also offers `live visualization'
of two dimensional sheets via output of images in the GIF format.  In addition,
the whole state is output regularly, to allow simulations to be resumed at a
later point in time.  This output usually stops all simulation whilst it is
being written to disk, time during which the processors are typically idle.  By
allowing multiple threads to output in parallel, the cost of outputting multiple
files can be reduced to the cost of outputting one.


\section{Experimental Protocols}

The toolkit developed provides a number of experimental protocols to use with
the cellular models to quantify the electrophysiological behaviour of the
modelled cells.  The provided protocols include the action potential duration at
90\% repolarisation (APD90) and the action potential (AP) profile; the
APD90 and APD50 restitution; the effective refractory period (ERP) restituion
(ERPr); the conduction velocity (CV) restitution (CVr); the temporal
vulnerability window to unidirectional conduction block (VW) and a flexible
system for specifying two dimensional sheet experiments, including the
initiation of re-entry via wavebreak protocols and computation of the spatial
vulnerability window.

\subsection{Action Potential Duration}

\subsection{Action Potential Duration Restitution}

The toolkit calculates the APDr via a standard S1--S2 protocol used in both
numerical simulations and also in physiological experiments.  The APDr is used
as a measure of how the cell responds to stimulations at different rates.  The
protocol used is shown here.


The cellular model is paced nine times with a stimulus close to the threshold
value at a given frequency or BCL.  At this point, the state is saved for the
paced cells.  The tenth S1 stimulus is then given, followed by the S2 after a
varying DI, which is reduced via an adaptive step to record the relationship
between DI and the APD of the following AP.  The toolkit also determines useful
parameters such as the maximal slope of the restitution curve, which can be
related to the stability of spiral waves within the tissue.  Both the APD90 and
the APD50 restitution can be calculated.

\subsection{Effective Refractory Period Restitution}

The ERPr is calculated by the toolkit using standard experimental protocols.
The ERP is defined as the shortest possible stimulus interval which still allows
for a successful AP.  A successful AP is defined as an AP which has an amplitude
of at least 80\% of the magnitude of a normal AP.  To determine the rate
dependence of the ERP, it is evaluated at a decreasing BCL.


The cellular model is paced seven times, and then the state is saved just before
the eighth AP is initiated.  The ERP for each chosen pacing rate is found via
binary search.  The low guess is typically chosen as zero, whilst the high guess
is the BCL being tested.  The eight AP is initiated immediately and then the S2
stimulus is delivered at the average of the guesses, as described previously.
If the ninth AP is successful, in addition to updating the high guess, the S2
interval is stored in a `best guess'.  With this small alteration, the binary
search algorithm is performed as described, with the saved state restored after
each iteration.

The reduction in BCL is stepped via an adaptive mechanism.  The BCL is reduced
until it is sufficiently short that the stimuli are occurring within the tail end
of the AP.

\subsection{Vulnerable Window}

The VW is principally a tool of numeric experimentation, based around a 1D ring
model of cardiac arrhythmia.  The VW is defined as the time period after a
stimulation during which a second stimulus elicits unidirectional propagation
along the strand.  In the case of a ring this causes retrograde propagation
which cycles endlessly.  If the stimulus is given too early, then the tissue
will still be refractory and no propagation of excitation will ensue.  If it is
given too late, then propagation will occur in both directions, which in the
ring case, results in the two excitation wavefronts annihilating each other.
Normal pacing could then resume.

The VW is found in a 1D strand model, set up as described previously.  The
strand is 200 units long and with a space step of 0.1~mm.  The first stimulus is
given to a 3 unit (0.3~mm) section at one end of the strand.  The test stimulus
is given to a 4 unit (0.4~mm) section, usually centred in the middle of the
strand. The VW is bounded on the lower side at the time when a stimulus any
earlier would result in no propagation from a test stimulus, and on the upper
side at the time when a stimulus any later would result in bidirectional
propagation from a test stimulus.  These three cases (no, unidirectional,
bidirectional) propagation are illustrated in figure TODO.

By counting the number of APs which cross the two ends of the strand--2, 3 and
4, respectively, for the three cases in figure TODO--the result of the test
stimulus can be determined.  The two boundaries of the VW are found via binary
search, with a lower guess of 0~ms and an upper guess of 1000~ms.  First, the
upper bound of the VW is determined by searching for the delay when 3 APs
crossing the ends becomes 4.  The lower bound is then found by looking for the
delay at which 3 APs crossing the ends of the strand become 2.  A minor
optimisation is performed when looking for the upper bound, in that the lower
bound's high and low guesses are updated at the same time, reducing the total
space which must be searched in the second half of the program.

\subsection{Conduction Velocity Restitution}

Measurement of CV (or solitary wave velocity) itself is performed both
experimentally and numerically, as is the minimum conduction interval, which is
the shortest interval between an S1 and an S2 stimulus which still travels the
length of the strand.  This is similar to the ERP, but can also be influenced by
factors which affect the inter-cellular coupling as well as heterogeneity in the
strand.  The CVr provides both the solitary wave velocity, the CV for a large S2
interval and the minimum stimulus interval, when the S2 interval gets too short
for an excitation wavefront to be fully propagated.

The CVr is found in a 1D strand model, set up as described previously.  The
strand is 200 units long with a space step of 0.1~mm.  Both the stimuli are
given to a 3 unit (0.3~mm) section at one end of the strand.  The conduction
velocity is estimated from the time taken to propagate between two points 10
units from the ends of the strand, to minimize the influence of boundary
effects.  The S2 time is then stepped via an adaptive step, until a second
excitation wave does not propagate.

\subsection{Spiral Wave Life Span}

Spiral Wave LS is examined experimentally and numerically.  The LS of the spiral
wave and the meander pattern of the tip are both used to gain insight into the
behaviour of the tissue under conditions of cardiac arrhythmia.

Spiral waves are initiated in a square sheet of tissue 375x375 nodes in
dimension with a space step of 0.1~mm, as described previously.  The tissue is
first stimulated along one edge via current injection of the first 3 rows of the
tissue.  The planar wave is then allowed to propagate over the tissue.  Some
time after the first wave is initiated, a second stimulus is applied.  The
second stimulus is applied to half the tissue, bisecting the first wave's
propagation front.  The second stimulus is a voltage clamp, with all the
included tissue clamped to a `high' potential, typically +20~mV--+50~mV for a
millisecond.  The generated spiral is then allowed to evolve until it
self-terminates, the spiral wave tip exits the tissue or until a sufficient
amount of time has passed such that the spiral can be classified as
`persistent'.  The time allowed for a wave to be classified as persistent is
typically 5 or 10~s.

The spiral wave tip traces are calculated via a standard contour based
algorithm, comparing the -60~mV contour line on snapshots of the electrical
activity 2.5~ms apart.

\subsection{Spatial Vulnerability Window}

The SVW is examined both experimentally and numerically.  It is used together
with the (Temporal) VW for quantifying a mutation or condition's potential for
arrhymogenesis.  It is defined as the smallest area of tissue in a 2D sheet,
which when given a threshold stimulus in the wake of a conditioning pulse causes
at least one `figure of eight' re-entry.  A figure of eight re-entry occurs when
the excitation waves from the tips of the test area propagate back through the
centre of the area, resulting in a pair of contra-rotating spiral waves at each
end of the region.

The sheet model used for the determination of the SVW can vary in size, as the
SVW can vary substantially, depending on the electrophysiology being simulated
by the cellular models at the nodes, but the smallest used is typically 375x375
nodes, with a spatial resolution of 0.1~mm.  The sheet is first given one
conditioning excitation, initiated by injecting a strip of nodes 3 nodes
(0.3~mm) in width with current along one edge of the sheet.  The wave is then
allowed to propagate through the tissue.  When the VW of the tissue is
positioned at the centre of the tissue, the test stimulus is delivered.  The
test stimulus is an area of tissue 20 nodes (2~mm) wide and of variable length.
After the test stimulus is delivered, the sheet is observed until figure of
eight re-entry is observed, or it is obvious that it will not occur.  The
protocol is then repeated with a test stimulus area of greater length.


\section{Results From Simulation Studies}

The cardiac simulation toolkit has been used in several simulation studies.
Here I present two such studies, representing different aspects of the toolkit.
The first is based on an experimental study which determined the existence of a
novel ion channel in the human atrium which caries an anion current through the
cellular membrane.  The second is principally a 2D study, concerning the effects
of Atrial Fibrillation induced Electrical Remodelling (AFER) and
electrophysiological heterogeneity in a 2D idealization of the right atrium and
the sino-atrial node.

\subsection{Anion Currents In The Human Atrium}

In a recent experimental study, Li et al.~\cite{li2007} determined the existence
of a novel outwardly rectifying anion current in human atrial myocytes isolated
from right atrial appendages taken from patients undergoing coronary bypass
surgery.  They determined that it was separate other known ionic currents and
performed preliminary modelling based on the CRN Human Atrial Myocyte
model~\cite{crn98}.  They determined the current could be modelled by
equation~\ref{anion:eqn} with the constants given in table~\ref{anion:table}.

The total current carried by the channel, $I_{ANION}$ was given by

\begin{equation}
\label{anion:eqn}
I_{ANION} = g_{ANION} \frac{V-E_{ANION}}{1-\left(c\times e^{\left(V-E_{ANION}\right)}\right)}
\end{equation}

where $g_{ANION}$ is the conductivity of the anion channel, $E_{ANION}$ is
the reversal potential of the channel and $c$ and $d$ are constants to
describe the behaviour.  All other symbols have their usual meanings.

The simulation study used the parameter set for the anion current carrying
$NO_{3^{-}}$ ions.  The effects of the addition of this current were quantified
via the experimental protocols outlined previously.  In the following
paragraphs, `control' is used to denote the unmodified CRN model and `anion' to
denote the CRN model with the additional current described by (\ref{anion:eqn})
with the $NO_{3^{-}}$ parameter set from Table \ref{anion:table}.

\subsubsection{Action Potential}

The AP generated by the control and anion simulations are shown in
figure~\ref{anion:ap}.  The AP profile shows a depressed plateau region (phase
2) and a slightly elevated resting potential.  There was no significant effect
on the $APD_{90}$ but the $APD_{50}$ is abbreviated by the presence of the anion
current, as shown in the Li et al.~\cite{li2007}

\subsubsection{Action Potential Duration Restitution}

The APDr curves produced by the control and anion cases are shown in
figure~\ref{anion:apd50} and \ref{anion:apd90}, for the restitution of
$APD_{50}$ and $APD_{90}$, respectively.  The $APD_{90}$ restitution is very
similar for the control and anion cases with both showing a large plateau region
where there is little changed, followed by a rapidly increasing slope as the DI
gets closer to 0~ms.  The maximal slopes are XXX in control and XXX in the anion
case.  By contrast, the $APD_{50}$ restitution for the anion case is depressed
when compared to the control case.

\subsubsection{Effective Refractory Period Restitution}


